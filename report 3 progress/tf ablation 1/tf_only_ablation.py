"""
Ablation study: Train models using ONLY TF features
to see if they have any predictive power at all.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score
)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print("=" * 80)
print("ABLATION: MODELS TRAINED ON TF FEATURES ONLY")
print("=" * 80)

# Define TF-only features
tf_features = ["regdb_tf_hits", "has_regdb_tf_site"]

print(f"\nFeatures: {tf_features}")
print(f"Number of features: {len(tf_features)}")

# Extract features and target
X_tf = pairs_df[tf_features].copy()
y_tf = pairs_df['Label'].astype(int)

print(f"\nDataset shape: {X_tf.shape}")
print(f"Feature statistics:")
for feat in tf_features:
    print(f"  {feat}:")
    print(f"    Mean: {X_tf[feat].mean():.4f}")
    print(f"    Non-zero: {(X_tf[feat] > 0).sum()} ({(X_tf[feat] > 0).sum()/len(X_tf)*100:.2f}%)")
    print(f"    Max: {X_tf[feat].max()}")

# Split data (same split as original)
X_train_tf, X_val_tf, y_train_tf, y_val_tf = train_test_split(
    X_tf, y_tf,
    test_size=0.15,
    stratify=y_tf,
    random_state=42
)

print(f"\nTrain: {len(X_train_tf)}, Val: {len(X_val_tf)}")

# Train simple models
print("\n" + "=" * 80)
print("TRAINING MODELS ON TF FEATURES ONLY")
print("=" * 80)

models_tf = {
    'Logistic Regression': LogisticRegression(
        max_iter=5000,
        class_weight='balanced',
        random_state=42
    ),
    'Random Forest': RandomForestClassifier(
        n_estimators=100,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    ),
    'XGBoost': XGBClassifier(
        n_estimators=100,
        scale_pos_weight=(y_train_tf.value_counts()[0] / y_train_tf.value_counts()[1]),
        random_state=42,
        n_jobs=-1,
        eval_metric='logloss'
    )
}

results_tf_only = {}

for name, model in models_tf.items():
    print(f"\nTraining {name}...")
    
    # Train
    model.fit(X_train_tf, y_train_tf)
    
    # Predict
    y_pred = model.predict(X_val_tf)
    y_pred_proba = model.predict_proba(X_val_tf)[:, 1]
    
    # Evaluate
    results_tf_only[name] = {
        'Accuracy': accuracy_score(y_val_tf, y_pred),
        'Precision': precision_score(y_val_tf, y_pred),
        'Recall': recall_score(y_val_tf, y_pred),
        'F1': f1_score(y_val_tf, y_pred),
        'ROC-AUC': roc_auc_score(y_val_tf, y_pred_proba)
    }
    
    print(f"  F1: {results_tf_only[name]['F1']:.3f}")
    print(f"  ROC-AUC: {results_tf_only[name]['ROC-AUC']:.3f}")

# Create comparison DataFrame
results_tf_df = pd.DataFrame(results_tf_only).T

print("\n" + "=" * 80)
print("RESULTS: TF FEATURES ONLY")
print("=" * 80)
print("\n", results_tf_df.round(3))

# Compare with full feature set
print("\n" + "=" * 80)
print("COMPARISON: TF ONLY vs FULL FEATURES")
print("=" * 80)

comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],
    'TF Only - F1': [results_tf_only[m]['F1'] for m in ['Logistic Regression', 'Random Forest', 'XGBoost']],
    'TF Only - ROC-AUC': [results_tf_only[m]['ROC-AUC'] for m in ['Logistic Regression', 'Random Forest', 'XGBoost']],
    'Full Features - F1': [results[m]['F1'] for m in ['Logistic Regression', 'Random Forest', 'XGBoost']],
    'Full Features - ROC-AUC': [results[m]['ROC-AUC'] for m in ['Logistic Regression', 'Random Forest', 'XGBoost']],
})

comparison['F1 Difference'] = comparison['Full Features - F1'] - comparison['TF Only - F1']
comparison['ROC-AUC Difference'] = comparison['Full Features - ROC-AUC'] - comparison['TF Only - ROC-AUC']

print("\n", comparison.round(3))

# Calculate baseline (predict majority class)
print("\n" + "=" * 80)
print("BASELINE: MAJORITY CLASS")
print("=" * 80)

majority_class = y_train_tf.mode()[0]
baseline_pred = np.full(len(y_val_tf), majority_class)
baseline_acc = accuracy_score(y_val_tf, baseline_pred)
baseline_f1 = f1_score(y_val_tf, baseline_pred)

print(f"\nMajority class: {majority_class}")
print(f"Baseline accuracy (always predict majority): {baseline_acc:.3f}")
print(f"Baseline F1: {baseline_f1:.3f}")

# Check if TF models beat baseline
print("\n" + "=" * 80)
print("DO TF FEATURES BEAT BASELINE?")
print("=" * 80)

for name in models_tf.keys():
    tf_f1 = results_tf_only[name]['F1']
    beats_baseline = tf_f1 > baseline_f1
    symbol = "âœ“" if beats_baseline else "âœ—"
    print(f"{symbol} {name}: F1 {tf_f1:.3f} vs baseline {baseline_f1:.3f} ({'BETTER' if beats_baseline else 'WORSE'})")

# Visualize (matching style of original ablation study)
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Plot 1: Side-by-side comparison (ROC-AUC)
ax = axes[0]
x = np.arange(len(comparison))
width = 0.35
bars1 = ax.bar(x - width/2, comparison['TF Only - ROC-AUC'], width, 
               label='TF Features Only', color='#ff7f0e', alpha=0.7, edgecolor='black')
bars2 = ax.bar(x + width/2, comparison['Full Features - ROC-AUC'], width,
               label='Full Features', color='#2ca02c', alpha=0.7, edgecolor='black')
ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.5)
ax.text(len(comparison)-0.5, 0.52, 'Random (0.5)', ha='right', fontsize=10, color='red')
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('ROC-AUC', fontsize=12)
ax.set_title('ROC-AUC: TF Features Only vs Full Features', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(comparison['Model'], rotation=0, ha='center')
ax.legend(loc='lower right')
ax.grid(axis='y', alpha=0.3)
ax.set_ylim([0, 1])

# Plot 2: Performance gain from adding all other features (horizontal bar chart)
ax = axes[1]
models_list = comparison['Model'].tolist()
roc_gains = comparison['ROC-AUC Difference'].tolist()

# Sort by gain (descending)
sorted_data = sorted(zip(models_list, roc_gains), key=lambda x: x[1], reverse=True)
models_sorted = [x[0] for x in sorted_data]
gains_sorted = [x[1] for x in sorted_data]

colors = ['#2ca02c' if g > 0 else '#d62728' for g in gains_sorted]
bars = ax.barh(range(len(models_sorted)), gains_sorted, color=colors, alpha=0.7, edgecolor='black')
ax.set_yticks(range(len(models_sorted)))
ax.set_yticklabels(models_sorted, fontsize=10)
ax.set_xlabel('ROC-AUC Gain from Adding All Other Features', fontsize=11)
ax.set_title('Performance Gain: Full Features vs TF Only\n(Positive = Other features help)', fontsize=13)
ax.axvline(x=0, color='black', linestyle='--', linewidth=1)
ax.grid(axis='x', alpha=0.3)

# Add value labels
for i, val in enumerate(gains_sorted):
    ax.text(val + 0.01, i, f'{val:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')

plt.tight_layout()
plt.savefig('graphs/tf_only_ablation.png', dpi=300, bbox_inches='tight')
print("\nðŸ’¾ Saved to 'graphs/tf_only_ablation.png'")
plt.show()

# Additional plot: Show TF features barely beat baseline
fig, ax = plt.subplots(figsize=(10, 6))

models_all = ['Baseline\n(Majority)'] + list(models_tf.keys()) + ['Full\nFeatures\n(XGBoost)']
f1_scores = [baseline_f1] + [results_tf_only[m]['F1'] for m in models_tf.keys()] + [results['XGBoost']['F1']]
colors_bars = ['#d62728'] + ['#ff7f0e']*3 + ['#2ca02c']

bars = ax.bar(range(len(models_all)), f1_scores, color=colors_bars, alpha=0.7, edgecolor='black', linewidth=1.5)
ax.set_ylabel('F1 Score', fontsize=13, fontweight='bold')
ax.set_title('TF Features vs Baseline vs Full Features\n(F1 Score Comparison)', fontsize=14, fontweight='bold')
ax.set_xticks(range(len(models_all)))
ax.set_xticklabels(models_all, fontsize=10)
ax.grid(axis='y', alpha=0.3)
ax.set_ylim([0, 1])

# Add value labels
for bar, score in zip(bars, f1_scores):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
            f'{score:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Add annotations
ax.axhline(y=baseline_f1, color='red', linestyle='--', linewidth=1, alpha=0.5)
ax.text(0.5, baseline_f1 + 0.05, f'Baseline: {baseline_f1:.3f}', fontsize=9, style='italic')

plt.tight_layout()
plt.savefig('graphs/tf_only_vs_baseline.png', dpi=300, bbox_inches='tight')
print("ðŸ’¾ Saved to 'graphs/tf_only_vs_baseline.png'")
plt.show()

# Final verdict
print("\n" + "=" * 80)
print("ðŸŽ¯ VERDICT")
print("=" * 80)

avg_tf_f1 = np.mean([results_tf_only[m]['F1'] for m in models_tf.keys()])
avg_full_f1 = np.mean([results[m]['F1'] for m in ['Logistic Regression', 'Random Forest', 'XGBoost']])

print(f"\nAverage F1 with TF features only: {avg_tf_f1:.3f}")
print(f"Average F1 with full features: {avg_full_f1:.3f}")
print(f"Improvement from adding other features: {avg_full_f1 - avg_tf_f1:.3f} ({(avg_full_f1/avg_tf_f1 - 1)*100:.1f}% relative gain)")

if avg_tf_f1 < baseline_f1 + 0.05:
    print("\nâŒ TF features have MINIMAL predictive power (barely beat baseline)")
    print("   â†’ Confirms extreme sparsity makes them unusable")
elif avg_tf_f1 < 0.6:
    print("\nâš ï¸  TF features have WEAK predictive power alone")
    print("   â†’ Need other features to be useful")
else:
    print("\nâœ“ TF features have SOME predictive power")
    print("   â†’ But much weaker than full feature set")

print("\nðŸ’¡ This confirms: TF features are not just redundant, they're uninformative!")
print("   The extreme sparsity (0.54%) makes them useless for supervised learning.")

